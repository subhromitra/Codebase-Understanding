{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nimport torch\n\nmodel_id = \"Qwen/Qwen2.5-Coder-7B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", \n                                             torch_dtype=torch.bfloat16, trust_remote_code=True)\nmodel.eval()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2) Building prompt for the BRAIN","metadata":{}},{"cell_type":"code","source":"def build_code_explanation_prompt(code_snippet, code_components=\"code\"):\n    return f\"\"\"You are a helpful AI assistant skilled in Python code understanding.\nAnalyze the following {code_components} and generate a concise explanation of what it does. \nJust provide explanation and nothing else. Begin explanation with this token <begin> & end with <end>\n\n```python\n{code_snippet}\nSummary:\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## ðŸ§ª Step 4: Run a Test\n\nfrom transformers import TextStreamer\n\ndef summarize_code(code_snippet, build_prompt=build_code_explanation_prompt):\n    prompt = build_prompt(code_snippet)\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    \n    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n    \n    output = model.generate(\n        **inputs,\n        streamer=streamer,\n        max_new_tokens=256,\n        temperature=0.0,\n        top_p=0.99,\n        do_sample=False\n    )\n    \n    return tokenizer.decode(output[0], skip_special_tokens=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_code = '''\ndef get_top_n_words(text, n=10):\n    words = text.lower().split()\n    freq = {}\n    for word in words:\n        freq[word] = freq.get(word, 0) + 1\n    return sorted(freq.items(), key=lambda x: x[1], reverse=True)[:n]\n'''\n\nsummary = summarize_code(example_code)\nprint(summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_code = \"\"\"\nfrom flask import Flask\n\napp = Flask(__name__)\n\n@app.route('/')\ndef hello():\n    return 'Hello, World!'\n\nif __name__ == '__main__':\n    app.run(debug=True)\n\n\"\"\"\n\nsummary = summarize_code(example_code)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_code = '''\ndef bubble_sort(numbers):\n    \"\"\"\n    Sorts a list of numbers using bubble sort algorithm.\n    \"\"\"\n    n = len(numbers)\n    for i in range(n):\n        for j in range(0, n-i-1):\n            if numbers[j] > numbers[j+1]:\n                numbers[j], numbers[j+1] = numbers[j+1], numbers[j]\n    return numbers\n\n'''\n\nsummary = summarize_code(example_code)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_code = '''\nimport torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\n\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\", output_hidden_states=True)\ntokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n\ninput_text = \"The quick brown fox\"\ninputs = tokenizer(input_text, return_tensors=\"pt\")\noutputs = model(**inputs)\n\nhidden_states = outputs.hidden_states  # Tuple of (layer+1) tensors\n\n# Apply LM head to each layer's last token hidden state\nfor i, hs in enumerate(hidden_states):\n    logits = model.lm_head(hs)  # shape: [batch, seq_len, vocab]\n    probs = torch.softmax(logits, dim=-1)\n    top_token_id = torch.argmax(probs[0, -1]).item()\n    top_token = tokenizer.decode(top_token_id)\n    print(f\"Layer {i}: Top token â†’ {top_token}\")\n\n'''\n\nsummary = summarize_code(example_code)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_code = '''\nimport ast\nfrom pathlib import Path\nfrom collections import defaultdict\n\nclass CodeFunction:\n    def __init__(self, name, file_path, lineno, source, calls):\n        self.name = name\n        self.file_path = file_path\n        self.lineno = lineno\n        self.source = source\n        self.calls = calls  # List of functions this one calls\n\n    def __repr__(self):\n        return f\"<Function {self.name} (calls: {self.calls})>\"\n\ndef extract_functions_from_file(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        source = f.read()\n\n    try:\n        tree = ast.parse(source)\n    except SyntaxError:\n        print(f\"Skipping file due to parse error: {file_path}\")\n        return []\n\n    funcs = []\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef):\n            name = node.name\n            lineno = node.lineno\n            calls = extract_calls_from_node(node)\n\n            # Grab raw source using line numbers\n            lines = source.splitlines()\n            end_line = max(getattr(n, 'lineno', lineno) for n in ast.walk(node))\n            func_source = \"\\n\".join(lines[lineno-1:end_line])\n\n            funcs.append(CodeFunction(\n                name=name,\n                file_path=str(file_path),\n                lineno=lineno,\n                source=func_source,\n                calls=calls\n            ))\n\n    return funcs\n\ndef extract_calls_from_node(node):\n    \"\"\"\n    Return a list of function names that are called inside this node.\n    Only tracks direct function calls (not methods for now).\n    \"\"\"\n    calls = []\n    for subnode in ast.walk(node):\n        if isinstance(subnode, ast.Call):\n            if isinstance(subnode.func, ast.Name):\n                calls.append(subnode.func.id)  # like func()\n            elif isinstance(subnode.func, ast.Attribute):\n                calls.append(subnode.func.attr)  # like obj.method()\n    return calls\n\ndef build_dependency_graph(codebase_path):\n    graph = defaultdict(list)      # function_name â†’ [called_function_names]\n    function_index = {}           # function_name â†’ CodeFunction object\n\n    py_files = Path(codebase_path).rglob(\"*.py\")\n\n    for file in py_files:\n        funcs = extract_functions_from_file(file)\n        for func in funcs:\n            function_index[func.name] = func\n            for callee in func.calls:\n                graph[func.name].append(callee)\n\n    return graph, function_index\n\nif __name__ == \"__main__\":\n    base_path = \"./my_project\"\n    graph, index = build_dependency_graph(base_path)\n\n    for func_name, callees in graph.items():\n        print(f\"{func_name} â†’ {callees}\")\n    \n    print(\"\\n=== Details for funcA ===\")\n    funcA = index.get(\"funcA\")\n    if funcA:\n        print(f\"File: {funcA.file_path}\")\n        print(f\"Source:\\n{funcA.source}\")\n\t\n'''\n\nsummary = summarize_code(example_code)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_code = '''\n\"\"\"\nThis program implements merge sort for a dictionary where:\n- Keys are real-valued numbers (e.g., employee salaries or IDs)\n- Values are instances of an `Employee` class (not defined here)\n\nThe `Employee` class is a custom-defined class representing an employee,\nwhich may contain fields such as name, age, department, role, etc.\nFor example:\n    class Employee:\n        def __init__(self, name, age, dept):\n            self.name = name\n            self.age = age\n            self.dept = dept\n\nfuncA(left: dict, right: dict) -> dict\n    - This function takes two sorted dictionaries (by keys) and merges them into a single sorted dictionary.\n    - It compares the keys in ascending order and appends the corresponding key-value pairs to a new dictionary.\n    - Since dictionaries are unordered in older Python versions, this function returns a sorted dictionary (Python 3.7+ maintains insertion order).\n\n\"\"\"\n\ndef funcB(data: dict) -> dict:\n    if len(data) <= 1:\n        return data\n\n    keys = list(data.keys())\n    mid = len(keys) // 2\n    left_keys = keys[:mid]\n    right_keys = keys[mid:]\n\n    left_dict = {k: data[k] for k in left_keys}\n    right_dict = {k: data[k] for k in right_keys}\n\n    sorted_left = funcB(left_dict)\n    sorted_right = funcB(right_dict)\n\n    return funcA(sorted_left, sorted_right)\n\n# funcA(left: dict, right: dict) -> dict` here\n# The funcA function should:\n# - Take two sorted dictionaries (by keys)\n# - Merge them into one sorted dictionary based on ascending key order\n# - Maintain the association between keys and their corresponding Employee objects\n\n'''\n\nsummary = summarize_code(example_code)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_code = \"\"\"\nfrom datetime import datetime\nimport random\n\nclass Logger:\n    @staticmethod\n    def log(message):\n        print(f\"[{datetime.now()}] {message}\")\n\nclass Employee:\n    def __init__(self, emp_id, name, age):\n        self.emp_id = emp_id\n        self.name = name\n        self.age = age\n        self.tasks = []\n    \n    def assign_task(self, task):\n        Logger.log(f\"Assigning task '{task.title}' to {self.name}\")\n        self.tasks.append(task)\n    \n    def complete_task(self, task_id):\n        for task in self.tasks:\n            if task.task_id == task_id:\n                task.mark_completed()\n                Logger.log(f\"{self.name} completed task '{task.title}'\")\n                return\n        Logger.log(f\"Task ID {task_id} not found for {self.name}\")\n    \n    def get_task_report(self):\n        return [(task.title, task.status) for task in self.tasks]\n\nclass Manager(Employee):\n    def __init__(self, emp_id, name, age, department):\n        super().__init__(emp_id, name, age)\n        self.department = department\n\n    def assign_task_to_employee(self, employee, task):\n        Logger.log(f\"Manager {self.name} assigning task '{task.title}' to {employee.name}\")\n        employee.assign_task(task)\n\n    def review_employee(self, employee):\n        completed = sum(1 for task in employee.tasks if task.status == \"Completed\")\n        total = len(employee.tasks)\n        Logger.log(f\"{employee.name} has completed {completed}/{total} tasks.\")\n        return completed, total\n\nclass Admin(Employee):\n    def __init__(self, emp_id, name, age):\n        super().__init__(emp_id, name, age)\n    \n    def reset_employee_tasks(self, employee):\n        Logger.log(f\"Admin {self.name} resetting tasks for {employee.name}\")\n        employee.tasks = []\n\nclass Department:\n    def __init__(self, name):\n        self.name = name\n        self.employees = []\n\n    def add_employee(self, employee):\n        Logger.log(f\"Adding {employee.name} to department {self.name}\")\n        self.employees.append(employee)\n\n    def list_employees(self):\n        return [emp.name for emp in self.employees]\n\nclass Task:\n    def __init__(self, task_id, title, description):\n        self.task_id = task_id\n        self.title = title\n        self.description = description\n        self.status = \"Assigned\"\n    \n    def mark_completed(self):\n        self.status = \"Completed\"\n\nclass SystemManager:\n    def __init__(self):\n        self.departments = []\n        self.employees = []\n\n    def create_department(self, name):\n        dept = Department(name)\n        self.departments.append(dept)\n        Logger.log(f\"Created department: {name}\")\n        return dept\n    \n    def hire_employee(self, name, age):\n        emp_id = f\"E{random.randint(1000, 9999)}\"\n        emp = Employee(emp_id, name, age)\n        self.employees.append(emp)\n        Logger.log(f\"Hired employee: {emp.name} (ID: {emp_id})\")\n        return emp\n\n    def assign_to_department(self, employee, department):\n        department.add_employee(employee)\n\n    def generate_report(self):\n        Logger.log(\"Generating full system report...\")\n        for dept in self.departments:\n            Logger.log(f\"Department: {dept.name}\")\n            for emp in dept.employees:\n                Logger.log(f\" - {emp.name}: {len(emp.tasks)} tasks assigned.\")\n\n# Simulated usage\nif __name__ == \"__main__\":\n    system = SystemManager()\n\n    # Setup\n    dev_dept = system.create_department(\"Development\")\n    qa_dept = system.create_department(\"QA\")\n\n    alice = system.hire_employee(\"Alice\", 28)\n    bob = system.hire_employee(\"Bob\", 32)\n    claire = Manager(\"M001\", \"Claire\", 40, dev_dept)\n\n    system.assign_to_department(alice, dev_dept)\n    system.assign_to_department(bob, qa_dept)\n    system.assign_to_department(claire, dev_dept)\n\n    # Tasks\n    t1 = Task(\"T101\", \"Fix login bug\", \"Resolve authentication issue.\")\n    t2 = Task(\"T102\", \"Write test cases\", \"Write unit tests for UserService.\")\n    claire.assign_task_to_employee(alice, t1)\n    claire.assign_task_to_employee(bob, t2)\n    alice.complete_task(\"T101\")\n    claire.review_employee(alice)\n    admin = Admin(\"A001\", \"Greg\", 50)\n    admin.reset_employee_tasks(alice)\n    system.generate_report()\n\"\"\"\n\nsummary = summarize_code(example_code)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3) Bug Hunting\n\nUnfortunately, this isnt happening.","metadata":{}},{"cell_type":"code","source":"def build_bug_hunter_prompt(code_snippet, code_components=\"code\"):\n    return f\"\"\"Analyze the following {code_components} and find if there are any bugs in the code. \n    Find all the bugs & fix them if any. Begin explanation with <begin> & end with <end>. Just provide\n    the bug fix nothing else. Bugs may be logical also.\n\n```python\n{code_snippet}\nSummary:\"\"\"\n\nexample_code = \"\"\"\nimport heapq\nclass Edge:\n    def __init__(self, to, rev, cap, cost):\n        self.to = to\n        self.rev = rev\n        self.cap = cap\n        self.cost = cost\n\nclass MinCostMaxFlow:\n    def __init__(self, N):\n        self.N = N\n        self.graph = [[] for _ in range(N)]\n\n    def add_edge(self, fr, to, cap, cost):\n        forward = Edge(to, len(self.graph[to]), cap, cost)\n        backward = Edge(fr, len(self.graph[fr]), 0, -cost)\n        self.graph[fr].append(forward)\n        self.graph[to].append(backward)\n\n    def min_cost_flow(self, s, t, maxf):\n        N = self.N\n        h = [0] * N\n        prevv = [0] * N\n        preve = [0] * N\n        INF = float('inf')\n        res = 0\n        flow = 0\n\n        def dijkstra():\n            dist = [INF] * N\n            used = [False] * N\n            dist[s] = 0\n            queue = [(0, s)]\n            while queue:\n                d, v = heapq.heappop(queue)\n                if used[v]:  \n                    continue\n                used[v] = True\n                for i, e in enumerate(self.graph[v]):\n                    if e.cap <= 0: \n                        continue\n                    new_cost = dist[v] + e.cost + h[v] - h[e.to]\n                    if dist[e.to] > new_cost:\n                        dist[e.to] = new_cost\n                        prevv[e.to] = v\n                        preve[e.to] = i\n                        heapq.heappush(queue, (dist[e.to], e.to))\n            return dist[t] != INF, dist\n\n        while flow < maxf:\n            found, dist = dijkstra()\n            if not found:\n                break\n            for v in range(N):\n                h[v] = dist[v] \n            d = maxf  \n            v = t\n            while v != s:\n                d = min(d, self.graph[prevv[v]][preve[v]].cap)\n                v = prevv[v]\n            flow += d\n            res += d * h[t]\n            v = t\n            while v != s:\n                e = self.graph[prevv[v]][preve[v]]\n                e.cap -= d\n                self.graph[e.to][e.rev].cap += d  \n                v = prevv[v]\n\n        return flow, res\n\n\"\"\"\n\nsummary = summarize_code(example_code, build_prompt=build_bug_hunter_prompt)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4) Line-by-line explanation","metadata":{}},{"cell_type":"code","source":"def build_line_by_line_explan_prompt(code_snippet, code_components=\"code\"):\n    return f\"\"\"You are a helpful AI assistant skilled in Python code understanding.\n    Analyze the following {code_components} and provide a line by line explanation of the code. \n\n```python\n{code_snippet}\nSummary:\"\"\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"example_code=\"\"\"\nfrom collections import deque, defaultdict\n\nclass Edge:\n    def __init__(self, to, rev, capacity):\n        self.to = to          # destination node\n        self.rev = rev        # index of reverse edge in the adjacency list\n        self.capacity = capacity  # current capacity\n\nclass Dinic:\n    def __init__(self, n):\n        self.n = n                              # number of nodes\n        self.graph = [[] for _ in range(n)]     # adjacency list of edges\n        self.level = [0] * n                    # level of each node\n        self.iter = [0] * n                     # current edge to explore for each node\n\n    def add_edge(self, fr, to, capacity):\n        forward = Edge(to, len(self.graph[to]), capacity)\n        backward = Edge(fr, len(self.graph[fr]), 0)  # reverse edge with 0 capacity\n        self.graph[fr].append(forward)\n        self.graph[to].append(backward)\n\n    def bfs(self, s, t):\n        self.level = [-1] * self.n\n        queue = deque([s])\n        self.level[s] = 0\n\n        while queue:\n            v = queue.popleft()\n            for edge in self.graph[v]:\n                if edge.capacity > 0 and self.level[edge.to] < 0:\n                    self.level[edge.to] = self.level[v] + 1\n                    queue.append(edge.to)\n\n        return self.level[t] != -1\n\n    def dfs(self, v, t, flow):\n        if v == t:\n            return flow\n        for i in range(self.iter[v], len(self.graph[v])):\n            edge = self.graph[v][i]\n            if edge.capacity > 0 and self.level[v] < self.level[edge.to]:\n                d = self.dfs(edge.to, t, min(flow, edge.capacity))\n                if d > 0:\n                    edge.capacity -= d\n                    self.graph[edge.to][edge.rev].capacity += d\n                    return d\n            self.iter[v] += 1\n        return 0\n\n    def max_flow(self, s, t):\n        flow = 0\n        INF = float('inf')\n        while self.bfs(s, t):\n            self.iter = [0] * self.n\n            f = self.dfs(s, t, INF)\n            while f > 0:\n                flow += f\n                f = self.dfs(s, t, INF)\n        return flow\n\"\"\"\n\nsummary = summarize_code(example_code, build_prompt=build_line_by_line_explan_prompt)\nprint(summary)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile resource_manager.py\n\nclass Logger:\n    @staticmethod\n    def log(message):\n        print(f\"[{datetime.now()}] {message}\")\n\nclass Employee:\n    def __init__(self, emp_id, name, age):\n        self.emp_id = emp_id\n        self.name = name\n        self.age = age\n        self.tasks = []\n    \n    def assign_task(self, task):\n        Logger.log(f\"Assigning task '{task.title}' to {self.name}\")\n        self.tasks.append(task)\n    \n    def complete_task(self, task_id):\n        for task in self.tasks:\n            if task.task_id == task_id:\n                task.mark_completed()\n                Logger.log(f\"{self.name} completed task '{task.title}'\")\n                return\n        Logger.log(f\"Task ID {task_id} not found for {self.name}\")\n    \n    def get_task_report(self):\n        return [(task.title, task.status) for task in self.tasks]\n\nclass Manager(Employee):\n    def __init__(self, emp_id, name, age, department):\n        super().__init__(emp_id, name, age)\n        self.department = department\n\n    def assign_task_to_employee(self, employee, task):\n        Logger.log(f\"Manager {self.name} assigning task '{task.title}' to {employee.name}\")\n        employee.assign_task(task)\n\n    def review_employee(self, employee):\n        completed = sum(1 for task in employee.tasks if task.status == \"Completed\")\n        total = len(employee.tasks)\n        Logger.log(f\"{employee.name} has completed {completed}/{total} tasks.\")\n        return completed, total\n\nclass Admin(Employee):\n    def __init__(self, emp_id, name, age):\n        super().__init__(emp_id, name, age)\n    \n    def reset_employee_tasks(self, employee):\n        Logger.log(f\"Admin {self.name} resetting tasks for {employee.name}\")\n        employee.tasks = []\n\nclass Department:\n    def __init__(self, name):\n        self.name = name\n        self.employees = []\n\n    def add_employee(self, employee):\n        Logger.log(f\"Adding {employee.name} to department {self.name}\")\n        self.employees.append(employee)\n\n    def list_employees(self):\n        return [emp.name for emp in self.employees]\n\nclass Task:\n    def __init__(self, task_id, title, description):\n        self.task_id = task_id\n        self.title = title\n        self.description = description\n        self.status = \"Assigned\"\n    \n    def mark_completed(self):\n        self.status = \"Completed\"\n\nclass SystemManager:\n    def __init__(self):\n        self.departments = []\n        self.employees = []\n\n    def create_department(self, name):\n        dept = Department(name)\n        self.departments.append(dept)\n        Logger.log(f\"Created department: {name}\")\n        return dept\n    \n    def hire_employee(self, name, age):\n        emp_id = f\"E{random.randint(1000, 9999)}\"\n        emp = Employee(emp_id, name, age)\n        self.employees.append(emp)\n        Logger.log(f\"Hired employee: {emp.name} (ID: {emp_id})\")\n        return emp\n\n    def assign_to_department(self, employee, department):\n        department.add_employee(employee)\n\n    def generate_report(self):\n        Logger.log(\"Generating full system report...\")\n        for dept in self.departments:\n            Logger.log(f\"Department: {dept.name}\")\n            for emp in dept.employees:\n                Logger.log(f\" - {emp.name}: {len(emp.tasks)} tasks assigned.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5) Parser + Dependency Graph Builder","metadata":{}},{"cell_type":"code","source":"import ast\nfrom pathlib import Path\nfrom collections import defaultdict\n\nclass CodeFunction:\n    def __init__(self, name, file_path, lineno, source, calls, parent_class=None):\n        self.name = name\n        self.file_path = file_path\n        self.lineno = lineno\n        self.source = source\n        self.calls = calls\n        self.parent_class = parent_class  # Optional class this function belongs to\n\n    def fqname(self):\n        return f\"{self.parent_class}.{self.name}\" if self.parent_class else self.name\n\n    def __repr__(self):\n        return f\"<Function {self.fqname()} (calls: {self.calls})>\"\n\nclass CodeClass:\n    def __init__(self, name, file_path, lineno, source, methods):\n        self.name = name\n        self.file_path = file_path\n        self.lineno = lineno\n        self.source = source\n        self.methods = methods  # List of CodeFunction objects\n\n    def __repr__(self):\n        return f\"<Class {self.name} (methods: {[m.name for m in self.methods]})>\"\n\n\t\t\ndef extract_classes_and_functions(file_path):\n    with open(file_path, 'r', encoding='utf-8') as f:\n        source = f.read()\n\n    try:\n        tree = ast.parse(source)\n    except SyntaxError:\n        print(f\"Skipping file due to parse error: {file_path}\")\n        return [], []\n\n    functions = []\n    classes = []\n    lines = source.splitlines()\n\n    for node in ast.walk(tree):\n        if isinstance(node, ast.FunctionDef) and isinstance(getattr(node, 'parent', None), ast.ClassDef):\n            continue  # Weâ€™ll handle class methods inside class parsing\n\n        if isinstance(node, ast.FunctionDef):\n            lineno = node.lineno\n            end_line = max(getattr(n, 'lineno', lineno) for n in ast.walk(node))\n            func_source = \"\\n\".join(lines[lineno-1:end_line])\n            calls = extract_calls_from_node(node)\n            functions.append(CodeFunction(\n                name=node.name,\n                file_path=str(file_path),\n                lineno=lineno,\n                source=func_source,\n                calls=calls,\n            ))\n\n        elif isinstance(node, ast.ClassDef):\n            class_lineno = node.lineno\n            class_end_line = max(getattr(n, 'lineno', class_lineno) for n in ast.walk(node))\n            class_source = \"\\n\".join(lines[class_lineno-1:class_end_line])\n\n            methods = []\n            for child in node.body:\n                if isinstance(child, ast.FunctionDef):\n                    lineno = child.lineno\n                    end_line = max(getattr(n, 'lineno', lineno) for n in ast.walk(child))\n                    method_source = \"\\n\".join(lines[lineno-1:end_line])\n                    calls = extract_calls_from_node(child)\n                    methods.append(CodeFunction(\n                        name=child.name,\n                        file_path=str(file_path),\n                        lineno=lineno,\n                        source=method_source,\n                        calls=calls,\n                        parent_class=node.name,\n                    ))\n\n            classes.append(CodeClass(\n                name=node.name,\n                file_path=str(file_path),\n                lineno=class_lineno,\n                source=class_source,\n                methods=methods\n            ))\n\n    return functions, classes\n\t\ndef extract_calls_from_node(node):\n    calls = []\n\n    class CallVisitor(ast.NodeVisitor):\n        def visit_Call(self, call_node):\n            # Extract the function name from different types of function calls\n            if isinstance(call_node.func, ast.Name):\n                # Simple function call: foo()\n                calls.append(call_node.func.id)\n            elif isinstance(call_node.func, ast.Attribute):\n                # Method or attribute call: obj.method()\n                # Try to extract just the attribute (method) name\n                calls.append(call_node.func.attr)\n            self.generic_visit(call_node)  # Continue visiting children\n\n    CallVisitor().visit(node)\n    return calls\n\n\t\ndef attach_parents(tree):\n    for node in ast.walk(tree):\n        for child in ast.iter_child_nodes(node):\n            child.parent = node\n\ndef resolve_fqname(name, function_index):\n    # If already fully qualified\n    if name in function_index:\n        return name\n    # Try to match by suffix\n    matches = [fq for fq in function_index if fq.endswith(f\".{name}\") or fq == name]\n    return matches[0] if matches else name  # fallback to original\n\n\n\t\t\t\ndef build_dependency_graph(codebase_path):\n    graph = defaultdict(list)\n    function_index = {}\n\n    py_files = list(Path(codebase_path).rglob(\"*.py\"))\n\n    all_funcs = []\n    all_methods = []\n\n    # --------- Pass 1: Build function index ---------\n    for file in py_files:\n        funcs, classes = extract_classes_and_functions(file)\n\n        for func in funcs:\n            key = func.fqname()\n            function_index[key] = func\n            all_funcs.append(func)\n\n        for cls in classes:\n            for method in cls.methods:\n                key = method.fqname()\n                function_index[key] = method\n                all_methods.append(method)\n\n    # --------- Pass 2: Build graph using full index ---------\n    for func in all_funcs:\n        key = func.fqname()\n        for callee in func.calls:\n            fq_callee = resolve_fqname(callee, function_index)\n            graph[key].append(fq_callee)\n\n    for method in all_methods:\n        key = method.fqname()\n        for callee in method.calls:\n            fq_callee = resolve_fqname(callee, function_index)\n            graph[key].append(fq_callee)\n\n    return graph, function_index\n\n\n\t\n\nbase_path = \".\"\ngraph, index = build_dependency_graph(base_path)\ngraph = {k: v for k, v in graph.items() if '.' in k}  # This is a temporary fix for the duplicate graph node. Need to handle code logic properly\n\nfor func_name, callees in graph.items():\n    print(f\"{func_name} â†’ {callees}\")\n    \n    #print(\"\\n=== Details for funcA ===\")\n    #funcA = index.get(\"funcA\")\n    #if funcA:\n    #    print(f\"File: {funcA.file_path}\")\n    #    print(f\"Source:\\n{funcA.source}\")\n\n\n\t\t","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6) Building the Context Assembler","metadata":{}},{"cell_type":"code","source":"def get_dependency_closure(func_name, graph):\n    visited = set()\n    stack = [func_name]\n\n    while stack:\n        current = stack.pop()\n        if current not in visited:\n            visited.add(current)\n            for dep in graph.get(current, []):\n                if dep not in visited:\n                    stack.append(dep)\n\n    return visited\n\nclosure = get_dependency_closure(\"SystemManager.hire_employee\", graph)\nprint(closure)\n# closure â†’ {'DataLoader.load', 'DataLoader._validate_input', 'parse_config', ...}\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"SystemManager.hire_employee\" in index)  # Should be True\nprint(index.get(\"SystemManager.hire_employee\"))  # Should not be None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"SystemManager.hire_employee\" in graph)\nprint(graph[\"SystemManager.hire_employee\"])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assemble_context(func_name, graph, function_index):\n    relevant_funcs = get_dependency_closure(func_name, graph)\n    relevant_funcs.add(func_name)  # Make sure the root function is included\n    \n    context_parts = []\n\n    for fname in sorted(relevant_funcs):\n        if fname in function_index:\n            func_obj = function_index[fname]\n            label = f\"--- {fname} (from {func_obj.file_path}) ---\"\n            context_parts.append(label)\n            context_parts.append(func_obj.source)\n            context_parts.append(\"\")  # blank line\n        else:\n            print(f\"[Warning] Missing in index: {fname}\")  # Debug print\n\n    return \"\\n\".join(context_parts)\n\n\ndef build_prompt_for_function(func_name, graph, function_index):\n    context = assemble_context(func_name, graph, function_index)\n    \n    prompt = f\"\"\"\nYou are an AI assistant helping a developer understand a Python function.\n\nBelow is the source code for the function `{func_name}` and its dependencies.\n\n{context}\n\nExplain clearly what `{func_name}` does in simple terms.\n\"\"\"\n    return prompt.strip()\n\nif __name__ == \"__main__\":\n    graph, function_index = build_dependency_graph(\".\")\n\n    func_name = \"SystemManager.hire_employee\"  # or just \"load_data\"\n    prompt = build_prompt_for_function(func_name, graph, function_index)\n\n    print(prompt)  # or send it to the LLM\n\n    summary = summarize_code(prompt)\n    print(summary)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# AGENTIC WRAPPER CLASSES","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# DISTILLING QWEN2.5-7B to QWEN2.5-1.5B model","metadata":{}},{"cell_type":"code","source":"!pip install accelerate --quiet\n!pip install -U bitsandbytes --quiet","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nimport torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom accelerate import Accelerator\nimport gc\nimport os\nimport logging\n\n# Set up logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n# Set environment variables for better memory management\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nclass CodeDataset:\n    def __init__(self, name='jtatman/python-code-dataset-500k', split='train', max_samples=None):\n        self.dataset = load_dataset(name, split=split)\n        if max_samples:\n            self.dataset = self.dataset.select(range(max_samples))\n        \n        # Pre-filter dataset to remove problematic entries\n        self.dataset = self.dataset.filter(self._is_valid_sample)\n        logger.info(f\"Dataset filtered to {len(self.dataset)} valid samples\")\n    \n    def _is_valid_sample(self, sample):\n        \"\"\"Filter out invalid samples\"\"\"\n        instruction = sample.get('instruction', '')\n        output = sample.get('output', '')\n        \n        # Check if both fields exist and are strings\n        if not isinstance(instruction, str) or not isinstance(output, str):\n            return False\n        \n        # Check if at least one field has meaningful content\n        if len(instruction.strip()) == 0 and len(output.strip()) == 0:\n            return False\n        \n        # Check for reasonable length (not too short or too long)\n        total_len = len(instruction) + len(output)\n        if total_len < 10 or total_len > 2048:\n            return False\n            \n        return True\n    \n    def get_data(self):\n        return self.dataset\n\nclass TeacherModel:\n    def __init__(self, model_name=\"Qwen/Qwen2.5-Coder-14B\", device='cuda'): #\"Qwen/Qwen2.5-Coder-7B\"\n        # Use 8-bit quantization for memory efficiency\n        bnb_config = BitsAndBytesConfig(\n            load_in_8bit=True,\n            llm_int8_enable_fp32_cpu_offload=True,\n            llm_int8_skip_modules=[\"lm_head\"],\n        )\n\n        # For maximum quality (slightly larger):\n        #bnb_config = BitsAndBytesConfig(\n        #    load_in_4bit=True,\n        #    bnb_4bit_quant_type=\"fp4\",  # Float4 instead of NormalFloat4\n        #    bnb_4bit_use_double_quant=True,\n        #    bnb_4bit_compute_dtype=torch.float16,\n        #)\n\n        # For maximum memory savings (may impact quality):\n        #bnb_config = BitsAndBytesConfig(\n        #    load_in_4bit=True,\n        #    bnb_4bit_quant_type=\"nf4\",\n        #    bnb_4bit_use_double_quant=True,\n        #    bnb_4bit_compute_dtype=torch.float16,\n        #    llm_int8_skip_modules=[\"lm_head\"],)  \n                                            # Keep output layer unquantized, # Common modules to keep unquantized for stability:\n                                            #llm_int8_skip_modules=[\"lm_head\", \"embed_tokens\", \"norm\"]\n\n                                            # Or just the critical ones:\n                                            # llm_int8_skip_modules=[\"lm_head\"]\n    \n\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side='left')\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n            \n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            quantization_config=bnb_config,\n            device_map='auto',\n            trust_remote_code=True,\n            torch_dtype=torch.float16,\n            low_cpu_mem_usage=True,\n        )\n        self.device = device\n        self.model.eval()\n\n    def forward(self, input_ids, attention_mask):\n        with torch.no_grad():\n            try:\n                outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = outputs.logits\n                \n                # Check for invalid values and handle them\n                if torch.isnan(logits).any() or torch.isinf(logits).any():\n                    logger.warning(\"NaN/Inf detected in teacher logits, applying fixes...\")\n                    logits = torch.where(torch.isnan(logits), torch.zeros_like(logits), logits)\n                    logits = torch.clamp(logits, min=-50.0, max=50.0)\n                \n                return logits.half()\n            except Exception as e:\n                logger.error(f\"Error in teacher forward pass: {e}\")\n                raise\n\nclass StudentModel:\n    def __init__(self, model_name=\"Qwen/Qwen2.5-Coder-1.5B\", device='cuda'):\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, padding_side='left')\n        if self.tokenizer.pad_token is None:\n            self.tokenizer.pad_token = self.tokenizer.eos_token\n        \n        self.model = AutoModelForCausalLM.from_pretrained(\n            model_name,\n            torch_dtype=torch.float16,\n            device_map='auto',\n            trust_remote_code=True,\n            low_cpu_mem_usage=True,\n        )\n        self.device = device\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        return outputs.logits\n\ndef prepare_text_pairs(batch):\n    \"\"\"Prepare instruction-output pairs with better formatting\"\"\"\n    texts = []\n    \n    for i in range(len(batch['instruction'])):\n        instruction = str(batch['instruction'][i]).strip()\n        output = str(batch['output'][i]).strip()\n        \n        # Create a properly formatted text pair\n        if instruction and output:\n            # Use a clear format that separates instruction from output\n            formatted_text = f\"# Instruction:\\n{instruction}\\n\\n# Output:\\n{output}\"\n        elif instruction:\n            formatted_text = f\"# Code:\\n{instruction}\"\n        elif output:\n            formatted_text = f\"# Code:\\n{output}\"\n        else:\n            continue  # Skip empty entries\n            \n        texts.append(formatted_text)\n    \n    return texts\n\ndef tokenize_batch(batch, tokenizer, max_length=512):\n    \"\"\"Improved tokenization with better error handling\"\"\"\n    try:\n        # Prepare texts\n        texts = prepare_text_pairs(batch)\n        \n        if not texts:\n            logger.warning(\"No valid texts found in batch\")\n            return None\n        \n        # Tokenize with better parameters\n        tokens = tokenizer(\n            texts,\n            padding='max_length',\n            truncation=True,\n            max_length=max_length,\n            return_tensors='pt',\n            add_special_tokens=True\n        )\n        \n        # Verify tokenization was successful\n        if tokens['input_ids'].size(0) == 0:\n            logger.warning(\"Tokenization resulted in empty batch\")\n            return None\n        \n        # Check for valid tokens (not all padding)\n        non_pad_tokens = (tokens['input_ids'] != tokenizer.pad_token_id).sum(dim=1)\n        if non_pad_tokens.min() < 5:  # At least 5 non-padding tokens\n            logger.warning(\"Batch contains sequences with too few valid tokens\")\n            return None\n        \n        return {\n            'input_ids': tokens['input_ids'],\n            'attention_mask': tokens['attention_mask']\n        }\n        \n    except Exception as e:\n        logger.error(f\"Error in tokenization: {e}\")\n        return None\n\ndef compute_distillation_loss(student_logits, teacher_logits, attention_mask, \n                            temperature=3.0, alpha=0.7, top_k=None):\n    \"\"\"\n    Compute a stable distillation loss combining KL divergence and cross-entropy\n    \"\"\"\n    # Ensure both logits are in float32 for numerical stability\n    student_logits = student_logits.float()\n    teacher_logits = teacher_logits.float()\n    \n    # Align dimensions\n    min_seq_len = min(student_logits.size(1), teacher_logits.size(1))\n    min_vocab_size = min(student_logits.size(-1), teacher_logits.size(-1))\n    \n    student_logits = student_logits[:, :min_seq_len, :min_vocab_size]\n    teacher_logits = teacher_logits[:, :min_seq_len, :min_vocab_size]\n    attention_mask = attention_mask[:, :min_seq_len]\n    \n    # Apply temperature scaling\n    student_logits_scaled = student_logits / temperature\n    teacher_logits_scaled = teacher_logits / temperature\n    \n    # Clamp logits to prevent extreme values\n    student_logits_scaled = torch.clamp(student_logits_scaled, min=-20, max=20)\n    teacher_logits_scaled = torch.clamp(teacher_logits_scaled, min=-20, max=20)\n    \n    # Compute soft targets from teacher\n    with torch.no_grad():\n        teacher_probs = torch.softmax(teacher_logits_scaled, dim=-1)\n        # Add small epsilon for numerical stability\n        teacher_probs = torch.clamp(teacher_probs, min=1e-8, max=1.0)\n    \n    # Compute student log probabilities\n    student_log_probs = torch.log_softmax(student_logits_scaled, dim=-1)\n    \n    # KL divergence loss (only on non-masked positions)\n    kl_loss = -torch.sum(teacher_probs * student_log_probs, dim=-1)\n    \n    # Apply attention mask and compute mean\n    mask = attention_mask.float()\n    masked_kl_loss = kl_loss * mask\n    \n    valid_tokens = mask.sum()\n    if valid_tokens > 0:\n        kl_loss_mean = masked_kl_loss.sum() / valid_tokens\n    else:\n        kl_loss_mean = torch.tensor(0.0, device=student_logits.device, requires_grad=True)\n    \n    # Standard cross-entropy loss for hard targets\n    teacher_hard_targets = torch.argmax(teacher_logits, dim=-1)\n    ce_loss = torch.nn.functional.cross_entropy(\n        student_logits.view(-1, student_logits.size(-1)),\n        teacher_hard_targets.view(-1),\n        reduction='none',\n        ignore_index=-100\n    ).view(student_logits.shape[:-1])\n    \n    masked_ce_loss = ce_loss * mask\n    if valid_tokens > 0:\n        ce_loss_mean = masked_ce_loss.sum() / valid_tokens\n    else:\n        ce_loss_mean = torch.tensor(0.0, device=student_logits.device, requires_grad=True)\n    \n    # Combine losses\n    total_loss = alpha * kl_loss_mean + (1 - alpha) * ce_loss_mean\n    \n    # Final safety checks\n    if torch.isnan(total_loss) or torch.isinf(total_loss):\n        logger.warning(\"NaN/Inf loss detected, returning safe fallback\")\n        return torch.tensor(1.0, device=student_logits.device, requires_grad=True)\n    \n    # Clamp final loss to reasonable range\n    total_loss = torch.clamp(total_loss, min=0.0, max=50.0)\n    \n    return total_loss\n\ndef safe_gradient_norm(model):\n    \"\"\"Compute gradient norm safely\"\"\"\n    total_norm = 0.0\n    param_count = 0\n    \n    for p in model.parameters():\n        if p.grad is not None:\n            param_norm = p.grad.data.norm(2).item()\n            if not (torch.isnan(torch.tensor(param_norm)) or torch.isinf(torch.tensor(param_norm))):\n                total_norm += param_norm ** 2\n                param_count += 1\n    \n    if param_count == 0:\n        return 0.0\n    \n    total_norm = (total_norm ** 0.5)\n    return total_norm\n\nclass DistillationTrainer:\n    def __init__(self, teacher, student, dataset, batch_size=2, grad_accum=8, \n                 learning_rate=2e-5, max_grad_norm=1.0, alpha=0.7):\n        self.teacher = teacher\n        self.student = student\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.grad_accum = grad_accum\n        self.max_grad_norm = max_grad_norm\n        self.alpha = alpha\n        \n        # Initialize accelerator\n        self.accelerator = Accelerator(\n            mixed_precision='fp16',\n            gradient_accumulation_steps=grad_accum\n        )\n        \n        # Optimizer with better settings\n        self.optimizer = torch.optim.AdamW(\n            self.student.model.parameters(),\n            lr=learning_rate,\n            betas=(0.9, 0.999),\n            eps=1e-8,\n            weight_decay=0.01\n        )\n        \n        # Learning rate scheduler\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n            self.optimizer, \n            T_max=len(dataset) // (batch_size * grad_accum),\n            eta_min=1e-6\n        )\n        \n        # Use student tokenizer for consistency\n        self.tokenizer = self.student.tokenizer\n        \n        # Prepare models\n        self.student.model, self.optimizer, self.scheduler = self.accelerator.prepare(\n            self.student.model, self.optimizer, self.scheduler\n        )\n        \n        # Statistics tracking\n        self.stats = {\n            'processed_batches': 0,\n            'skipped_batches': 0,\n            'total_loss': 0.0,\n            'high_grad_norm_count': 0\n        }\n        \n        logger.info(f\"Trainer initialized with batch_size={batch_size}, grad_accum={grad_accum}\")\n        logger.info(f\"Effective batch size: {batch_size * grad_accum}\")\n\n    def train(self, epochs=1, save_steps=500):\n        # Create dataloader with better settings\n        data_loader = DataLoader(\n            self.dataset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            num_workers=2,\n            pin_memory=True,\n            drop_last=True  # Ensure consistent batch sizes\n        )\n        data_loader = self.accelerator.prepare(data_loader)\n        \n        self.student.model.train()\n        \n        for epoch in range(epochs):\n            logger.info(f\"Starting epoch {epoch + 1}/{epochs}\")\n            \n            progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\")\n            \n            for step, batch in enumerate(progress_bar):\n                try:\n                    with self.accelerator.accumulate(self.student.model):\n                        # Tokenize batch\n                        tokenized = tokenize_batch(batch, self.tokenizer, max_length=512)\n                        \n                        if tokenized is None:\n                            self.stats['skipped_batches'] += 1\n                            continue\n                        \n                        input_ids = tokenized['input_ids'].to(self.accelerator.device)\n                        attention_mask = tokenized['attention_mask'].to(self.accelerator.device)\n                        \n                        # Get teacher predictions\n                        with torch.no_grad():\n                            teacher_logits = self.teacher.forward(input_ids, attention_mask)\n                        \n                        # Student forward pass\n                        with self.accelerator.autocast():\n                            student_outputs = self.student.model(\n                                input_ids=input_ids,\n                                attention_mask=attention_mask\n                            )\n                            student_logits = student_outputs.logits\n                            \n                            # Compute distillation loss\n                            loss = compute_distillation_loss(\n                                student_logits, teacher_logits, attention_mask,\n                                temperature=3.0, alpha=self.alpha\n                            )\n                        \n                        # Backward pass\n                        self.accelerator.backward(loss)\n                        \n                        # Gradient clipping and optimization\n                        if self.accelerator.sync_gradients:\n                            grad_norm = safe_gradient_norm(self.student.model)\n                            \n                            if grad_norm > self.max_grad_norm:\n                                self.accelerator.clip_grad_norm_(\n                                    self.student.model.parameters(), \n                                    self.max_grad_norm\n                                )\n                                self.stats['high_grad_norm_count'] += 1\n                            \n                            self.optimizer.step()\n                            self.scheduler.step()\n                            self.optimizer.zero_grad()\n                        \n                        # Update statistics\n                        self.stats['processed_batches'] += 1\n                        self.stats['total_loss'] += loss.item()\n                        \n                        # Update progress bar\n                        if self.stats['processed_batches'] > 0:\n                            avg_loss = self.stats['total_loss'] / self.stats['processed_batches']\n                            current_lr = self.scheduler.get_last_lr()[0]\n                            \n                            progress_info = {\n                                'loss': f'{loss.item():.4f}',\n                                'avg_loss': f'{avg_loss:.4f}',\n                                'lr': f'{current_lr:.2e}',\n                                'processed': self.stats['processed_batches'],\n                                'skipped': self.stats['skipped_batches'],\n                                'grad_clips': self.stats['high_grad_norm_count']\n                            }\n                            progress_bar.set_postfix(progress_info)\n                        \n                        # Periodic cleanup\n                        if step % 50 == 0:\n                            torch.cuda.empty_cache()\n                            \n                except Exception as e:\n                    logger.error(f\"Error at step {step}: {e}\")\n                    self.stats['skipped_batches'] += 1\n                    torch.cuda.empty_cache()\n                    continue\n            \n            # End of epoch summary\n            self._print_epoch_summary(epoch + 1)\n            \n            # Cleanup\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    def _print_epoch_summary(self, epoch):\n        \"\"\"Print detailed epoch summary\"\"\"\n        total_batches = self.stats['processed_batches'] + self.stats['skipped_batches']\n        success_rate = (self.stats['processed_batches'] / total_batches * 100) if total_batches > 0 else 0\n        avg_loss = (self.stats['total_loss'] / self.stats['processed_batches']) if self.stats['processed_batches'] > 0 else 0\n        \n        logger.info(f\"\\nEpoch {epoch} Summary:\")\n        logger.info(f\"  Processed Batches: {self.stats['processed_batches']}\")\n        logger.info(f\"  Skipped Batches: {self.stats['skipped_batches']}\")\n        logger.info(f\"  Success Rate: {success_rate:.1f}%\")\n        logger.info(f\"  Average Loss: {avg_loss:.4f}\")\n        logger.info(f\"  Gradient Clips: {self.stats['high_grad_norm_count']}\")\n        logger.info(f\"  Current LR: {self.scheduler.get_last_lr()[0]:.2e}\")\n\n    def save_model(self, output_dir=\"distil-Qwen2.5-Coder-1.5B\"):\n        \"\"\"Save the distilled model\"\"\"\n        self.accelerator.wait_for_everyone()\n        unwrapped_model = self.accelerator.unwrap_model(self.student.model)\n        \n        if self.accelerator.is_main_process:\n            unwrapped_model.save_pretrained(output_dir)\n            self.tokenizer.save_pretrained(output_dir)\n            logger.info(f\"Model saved to {output_dir}\")\n\ndef main():\n    # Setup\n    torch.cuda.empty_cache()\n    gc.collect()\n    \n    # GPU info\n    logger.info(f\"Available GPUs: {torch.cuda.device_count()}\")\n    for i in range(torch.cuda.device_count()):\n        props = torch.cuda.get_device_properties(i)\n        logger.info(f\"GPU {i}: {props.name} - {props.total_memory/1e9:.1f}GB\")\n    \n    # Load dataset with better filtering\n    logger.info(\"Loading and filtering dataset...\")\n    dataset_loader = CodeDataset(max_samples=50000)  # 50K samples\n    dataset = dataset_loader.get_data()\n    logger.info(f\"Final dataset size: {len(dataset)} samples\")\n    \n    # Load models\n    logger.info(\"Loading teacher model...\")\n    teacher = TeacherModel(\"Qwen/Qwen2.5-Coder-7B\") # Qwen/Qwen2.5-Coder-14B-Instruct\n    \n    logger.info(\"Loading student model...\")\n    student = StudentModel()\n    \n    logger.info(f\"Teacher vocab size: {len(teacher.tokenizer)}\")\n    logger.info(f\"Student vocab size: {len(student.tokenizer)}\")\n    \n    # Initialize trainer\n    trainer = DistillationTrainer(\n        teacher=teacher,\n        student=student,\n        dataset=dataset,\n        batch_size=64,\n        grad_accum=1,\n        learning_rate=2e-5,\n        max_grad_norm=1.0,\n        alpha=0.8\n    )\n    \n    # Train\n    logger.info(\"Starting training...\")\n    trainer.train(epochs=2)\n    \n    # Save model\n    trainer.save_model(\"distil-Qwen2.5-Coder-1.5B-Instruct\")\n    logger.info(\"Training completed!\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T12:07:22.990633Z","iopub.execute_input":"2025-08-20T12:07:22.990913Z","iopub.status.idle":"2025-08-20T16:56:01.975006Z","shell.execute_reply.started":"2025-08-20T12:07:22.990891Z","shell.execute_reply":"2025-08-20T16:56:01.974112Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"2025-08-20 12:07:33.374894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755691653.397350     303 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755691653.404093     303 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1376c35868714f9ea097c02cd5ec0710"}},"metadata":{}},{"name":"stderr","text":"Epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 513/513 [2:23:06<00:00, 16.74s/it]  \nEpoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 513/513 [2:23:55<00:00, 16.83s/it]  \n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"1) Using 14B teacher @4-bit precision. Student model 1.5B @FP-16 precision. Below is the GPU usage for the distillation task that I performed.\nThis setup takes 9hrs per epoch. Batch size = 16\n\n2) Using 7B teacher @4-bit precision takes 4hrs per epoch. Batch size = 16.\n\n3) Using 7B teacher @8-bit precision takes 2hrs 20mins per epoch. Batch size = 64.\n\n\nToDos :\n\na) Use the neulab/conala dataset to find how good the distilled model is in comparison to 1.5B coder , 3B coder & 7B coder model. Use 14B Qwen2.5 coder & Gpt-OSS-20B in a LLM-as-a-Judge setup.","metadata":{}},{"cell_type":"code","source":"!nvidia-smi","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"teacher.device()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}